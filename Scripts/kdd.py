# -*- coding: utf-8 -*-
"""kdd.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CP2t-gIrSpZifU7VP457AR52dGk9dHf_
"""




import warnings
warnings.filterwarnings("ignore")
import itertools
from sklearn.preprocessing import MinMaxScaler
import numpy as np
import pandas as pd
from keras.utils.data_utils import get_file

# Downloading training and test sets to local drive
try:
    training_set_path = get_file('KDDTrain%2B.csv', origin='https://raw.githubusercontent.com/defcom17/NSL_KDD/master/KDDTrain%2B.csv')
except:
    print('Error downloading')
    raise


try:
    test_set_path = get_file('KDDTest%2B.csv', origin='https://raw.githubusercontent.com/defcom17/NSL_KDD/master/KDDTest%2B.csv')
except:
    print('Error downloading')
    raise
training_df = pd.read_csv(training_set_path, header=None)
testing_df = pd.read_csv(test_set_path, header=None)

training_df.head()

testing_df.head()

columns = [
    'duration',
    'protocol_type',
    'service',
    'flag',
    'src_bytes',
    'dst_bytes',
    'land',
    'wrong_fragment',
    'urgent',
    'hot',
    'num_failed_logins',
    'logged_in',
    'num_compromised',
    'root_shell',
    'su_attempted',
    'num_root',
    'num_file_creations',
    'num_shells',
    'num_access_files',
    'num_outbound_cmds',
    'is_host_login',
    'is_guest_login',
    'count',
    'srv_count',
    'serror_rate',
    'srv_serror_rate',
    'rerror_rate',
    'srv_rerror_rate',
    'same_srv_rate',
    'diff_srv_rate',
    'srv_diff_host_rate',
    'dst_host_count',
    'dst_host_srv_count',
    'dst_host_same_srv_rate',
    'dst_host_diff_srv_rate',
    'dst_host_same_src_port_rate',
    'dst_host_srv_diff_host_rate',
    'dst_host_serror_rate',
    'dst_host_srv_serror_rate',
    'dst_host_rerror_rate',
    'dst_host_srv_rerror_rate',
    'outcome',
    'difficulty'
]
training_df.columns = columns
testing_df.columns = columns

print("Training set has {} rows.".format(len(training_df)))
print("Testing set has {} rows.".format(len(testing_df)))

training_outcomes=training_df["outcome"].unique()
testing_outcomes=testing_df["outcome"].unique()
print("The training set has {} possible outcomes \n".format(len(training_outcomes)) )
print(", ".join(training_outcomes)+".")
print("\nThe testing set has {} possible outcomes \n".format(len(testing_outcomes)))
print(", ".join(testing_outcomes)+".")

# A list ot attack names that belong to each general attack type
dos_attacks=["snmpgetattack","back","land","neptune","smurf","teardrop","pod","apache2","udpstorm","processtable","mailbomb"]
r2l_attacks=["snmpguess","worm","httptunnel","named","xlock","xsnoop","sendmail","ftp_write","guess_passwd","imap","multihop","phf","spy","warezclient","warezmaster"]
u2r_attacks=["sqlattack","buffer_overflow","loadmodule","perl","rootkit","xterm","ps"]
probe_attacks=["ipsweep","nmap","portsweep","satan","saint","mscan"]

# Our new labels
classes=["Normal","Dos","R2L","U2R","Probe"]

#Helper function to label samples to 5 classes
def label_attack (row):
    if row["outcome"] in dos_attacks:
        return classes[1]
    if row["outcome"] in r2l_attacks:
        return classes[2]
    if row["outcome"] in u2r_attacks:
        return classes[3]
    if row["outcome"] in probe_attacks:
        return classes[4]
    return classes[0]


#We combine the datasets temporarily to do the labeling
test_samples_length = len(testing_df)
df=pd.concat([training_df,testing_df])
df["Class"]=df.apply(label_attack,axis=1)


# The old outcome field is dropped since it was replaced with the Class field, the difficulty field will be dropped as well.
df=df.drop("outcome",axis=1)
df=df.drop("difficulty",axis=1)

# we again split the data into training and test sets.
type_testing_df = testing_df
training_df= df.iloc[:-test_samples_length, :]
testing_df= df.iloc[-test_samples_length:,:]

training_outcomes=training_df["Class"].unique()
testing_outcomes=testing_df["Class"].unique()
print("The training set has {} possible outcomes \n".format(len(training_outcomes)) )
print(", ".join(training_outcomes)+".")
print("\nThe testing set has {} possible outcomes \n".format(len(testing_outcomes)))
print(", ".join(testing_outcomes)+".")

# Helper function for scaling continous values
def minmax_scale_values(training_df,testing_df, col_name):
    scaler = MinMaxScaler()
    scaler = scaler.fit(training_df[col_name].values.reshape(-1, 1))
    train_values_standardized = scaler.transform(training_df[col_name].values.reshape(-1, 1))
    training_df[col_name] = train_values_standardized
    test_values_standardized = scaler.transform(testing_df[col_name].values.reshape(-1, 1))
    testing_df[col_name] = test_values_standardized


#Helper function for one hot encoding
def encode_text(training_df,testing_df, name):
    training_set_dummies = pd.get_dummies(training_df[name])
    testing_set_dummies = pd.get_dummies(testing_df[name])
    for x in training_set_dummies.columns:
        dummy_name = "{}_{}".format(name, x)
        training_df[dummy_name] = training_set_dummies[x]
        if x in testing_set_dummies.columns :
            testing_df[dummy_name]=testing_set_dummies[x]
        else :
            testing_df[dummy_name]=np.zeros(len(testing_df))
    training_df.drop(name, axis=1, inplace=True)
    testing_df.drop(name, axis=1, inplace=True)


sympolic_columns=["protocol_type","service","flag"]
label_column="Class"
for column in df.columns :
    if column in sympolic_columns:
        encode_text(training_df,testing_df,column)
    elif not column == label_column:
        minmax_scale_values(training_df,testing_df, column)

training_df.head(5)

training_df.columns

# Assuming 'data' is the DataFrame
column_names = training_df.columns.tolist()

# Display all column names
for name in column_names:
    print(name)
np.save('../Data/kdd_after_features.npy', column_names)

x,y=training_df,training_df.pop("Class").values
x=x.values
x_test,y_test=testing_df,testing_df.pop("Class").values
x_test=x_test.values
y0=np.ones(len(y),np.int8)
y0[np.where(y==classes[0])]=0
y0_test=np.ones(len(y_test),np.int8)
y0_test[np.where(y_test==classes[0])]=0
input_shape = x.shape[1]

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc
from tensorflow import keras

# Define the autoencoder architecture with additional layers
input_dim = x.shape[1]
encoding_dim = 100  # Adjust the size of the encoding layer as per your requirements

input_data = keras.Input(shape=(input_dim,))
encoded = keras.layers.Dense(256, activation='relu')(input_data)
dropout_encoded = keras.layers.Dropout(0.1)(encoded)
encoded2 = keras.layers.Dense(128, activation='relu')(dropout_encoded)
dropout_encoded2 = keras.layers.Dropout(0.1)(encoded2)
encoded3 = keras.layers.Dense(64, activation='relu')(dropout_encoded2)
dropout_encoded3 = keras.layers.Dropout(0.1)(encoded3)
encoded4 = keras.layers.Dense(128, activation='relu')(dropout_encoded3)
dropout_encoded4 = keras.layers.Dropout(0.1)(encoded4)
encoded5 = keras.layers.Dense(256, activation='relu')(dropout_encoded4)
dropout_encoded5 = keras.layers.Dropout(0.1)(encoded5)
decoded = keras.layers.Dense(input_dim, activation='sigmoid')(dropout_encoded5)

autoencoder = keras.Model(input_data, decoded)

from tensorflow.keras.optimizers import Adam

# Assuming 'autoencoder' is the model you want to compile
learning_rate = 0.001  # Learning rate value

# Create an optimizer with the desired learning rate
optimizer = Adam(learning_rate=learning_rate)
autoencoder.compile(optimizer=optimizer, loss='mse')
# Train the autoencoder
autoencoder.fit(x, x, epochs=1, batch_size=5000)

# Save the model
model_path = "../Models/autoencoder_model.h5"
autoencoder.save(model_path)
print("Model saved.")

# Load the model
loaded_model = keras.models.load_model(model_path)
print("Model loaded.")

# Use the loaded model for anomaly detection
reconstructed_data = loaded_model.predict(x_test)
mse = np.mean(np.power(x_test - reconstructed_data, 2), axis=1)  # Compute mean squared error

# Compute ROC curve and AUC
fpr, tpr, thresholds = roc_curve(y0_test, mse)
roc_auc = auc(fpr, tpr)

# Plot ROC curve
plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (AUC = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")
plt.show()

x_train = np.reshape(x, (x.shape[0],1,x.shape[1]))
x_train.shape
x_test = np.reshape(x_test, (x_test.shape[0],1,x_test.shape[1]))
x_test.shape

# LSTM requirements
from tensorflow import keras
from keras.layers import LSTM
from keras.layers import Input
from keras.models import Model
from keras.layers import Dense # importing dense layer
from keras.models import Sequential #importing Sequential layer

lst = Sequential()
# input layer and LSTM layer with 50 neurons
lst.add(LSTM(50,input_dim=122))

# outpute layer with sigmoid activation
lst.add(Dense(1,activation='sigmoid'))
lst.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])
lst.summary()
history = lst.fit(x_train, y0, epochs=100, batch_size=5000,validation_split=0.2)
test_results = lst.evaluate(x_test, y0_test, verbose=1)
print(f'Test results - Loss: {test_results[0]} - Accuracy: {test_results[1]*100}%')
# Save the model
model_path = "../Models/lstm_model.h5"
lst.save(model_path)
print("Model saved.")

# Load the model
new_model = keras.models.load_model(model_path)

# Check its architecture
new_model.summary()
print("Model loaded.")

import matplotlib.pyplot as plt
# Plot of accuracy vs epoch of train and test dataset
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title("Plot of accuracy vs epoch for train and test dataset")
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='best')
plt.show()

# Prepare data for explanations.
# Make predictions
predicted_probabilities = lst.predict(x_test)
predicted_labels = (predicted_probabilities >= 0.5).astype(int)

rnn_false_positives = []  # Store indices of false positives
rnn_false_negatives = []  # Store indices of false negatives
for i in range(len(predicted_labels)):
    if predicted_labels[i][0] != y0_test[i]:
        if predicted_labels[i][0] == 1:  # False positive
            rnn_false_positives.append(i)
        else:
           rnn_false_negatives.append(i)

#rnn_false_positives

#rnn_false_negatives

kdd_selected_fp_rows = type_testing_df.loc[570:571]
kdd_selected_fp_rows_122 = pd.DataFrame(x_test[570:572].reshape(2,122))
kdd_selected_fp_rows.to_csv('../Data/kdd_selected_fp_rows.csv', index=True)
kdd_selected_fp_rows_122.to_csv('../Data/kdd_selected_fp_rows_122.csv', index=True)

# 22439 false postives  udp private normal
# 571 0.69

predicted_probabilities[571]

y0_test[571]

predicted_labels[571][0]

kdd_selected_fp_rows

y0_test[21930]

predicted_labels[21930][0]

kdd_selected_fn_rows = type_testing_df.loc[21920:21930]
kdd_selected_fn_rows_122 = pd.DataFrame(x_test[21920:21931].reshape(11,122))
kdd_selected_fn_rows.to_csv('../Data/kdd_selected_fn_rows.csv', index=True)
kdd_selected_fn_rows_122.to_csv('../Data/kdd_selected_fn_rows_122.csv', index=True)

predicted_probabilities[21930]

kdd_selected_fn_rows

kdd_selected_fn_rows_122
